{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0b83cf7",
   "metadata": {},
   "source": [
    "#  LLM на CPU проверка моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb57af4",
   "metadata": {},
   "source": [
    "\n",
    "- Проверить загрузку и генерацию 5 небольших моделей на CPU.\n",
    "-  Провести простую оценку качества (BLEU, ROUGE) на двух коротких примерах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1468f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка зависимостей\n",
    "!pip install llama-cpp-python transformers evaluate pandas matplotlib seaborn psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce457950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import psutil\n",
    "from llama_cpp import Llama\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# Список моделей\n",
    "models = [\n",
    "    {'name': 'tinyLLaMA-1B (Q4)', 'loader': 'llama_cpp', 'path': './models/tinyllama-1b-chat.Q4_K_M.gguf'},\n",
    "    {'name': 'Qwen-1B-small', 'loader': 'transformers', 'model_id': 'Qwen/Qwen-1B-small'},\n",
    "    {'name': 'GPT2 (124M)', 'loader': 'transformers', 'model_id': 'gpt2'},\n",
    "    {'name': 'GPT-Neo 125M', 'loader': 'transformers', 'model_id': 'EleutherAI/gpt-neo-125M'},\n",
    "    {'name': 'Bloom 560M', 'loader': 'transformers', 'model_id': 'bigscience/bloom-560m'}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10f0b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Замер времени загрузки и генерации\n",
    "results = []\n",
    "prompt = 'The quick brown fox jumps over the lazy dog.'\n",
    "for m in models:\n",
    "    rec = {'model': m['name'], 'loaded': False, 'load_time': None, 'gen_time': None}\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        if m['loader'] == 'llama_cpp':\n",
    "            mdl = Llama(model_path=m['path'], n_threads=4)\n",
    "            rec['load_time'] = time.time() - t0\n",
    "            rec['loaded'] = True\n",
    "            t1 = time.time()\n",
    "            out = mdl(prompt, max_tokens=32)\n",
    "            rec['gen_time'] = time.time() - t1\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(m['model_id'])\n",
    "            model = AutoModelForCausalLM.from_pretrained(m['model_id']).to('cpu')\n",
    "            rec['load_time'] = time.time() - t0\n",
    "            rec['loaded'] = True\n",
    "            t1 = time.time()\n",
    "            inputs = tokenizer(prompt, return_tensors='pt')\n",
    "            gen = model.generate(**inputs, max_new_tokens=32)\n",
    "            rec['gen_time'] = time.time() - t1\n",
    "    except Exception as e:\n",
    "        rec['error'] = str(e)\n",
    "    results.append(rec)\n",
    "df = pd.DataFrame(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0552f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация load_time и gen_time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "sns.barplot(data=df, x='model', y='load_time', ax=axes[0])\n",
    "axes[0].set_title('Load Time (s)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "sns.barplot(data=df, x='model', y='gen_time', ax=axes[1])\n",
    "axes[1].set_title('Generation Time (s)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec8405",
   "metadata": {},
   "source": [
    "## Оценка качества генерации\n",
    "Проводим простую оценку на двух коротких примерах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a4817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "examples = [\n",
    "    ('Translate to French: Hello, how are you?', 'Bonjour, comment ça va ?'),\n",
    "    ('Summarize: Tiny models enable fast inference.', 'Tiny models allow quick local inference.')\n",
    "]\n",
    "bleu = evaluate.load('bleu')\n",
    "rouge = evaluate.load('rouge')\n",
    "qual_results = []\n",
    "for m in models:\n",
    "    if not m.get('loaded', True):\n",
    "        continue\n",
    "    preds = []\n",
    "    refs = []\n",
    "    # Получение предсказаний\n",
    "    if m['loader'] == 'llama_cpp':\n",
    "        mdl = Llama(model_path=m['path'], n_threads=4)\n",
    "        for inp, ref in examples:\n",
    "            out = mdl(inp, max_tokens=64)['choices'][0]['text']\n",
    "            preds.append(out)\n",
    "            refs.append([ref])\n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(m['model_id'])\n",
    "        model = AutoModelForCausalLM.from_pretrained(m['model_id']).to('cpu')\n",
    "        for inp, ref in examples:\n",
    "            inpt = tokenizer(inp, return_tensors='pt')\n",
    "            gen = model.generate(**inpt, max_new_tokens=64)\n",
    "            out = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "            preds.append(out)\n",
    "            refs.append([ref])\n",
    "    # Вычисление метрик\n",
    "    bleu_score = bleu.compute(predictions=preds, references=refs)['bleu']\n",
    "    rouge_score = rouge.compute(predictions=preds, references=refs)['rouge']['rougeL']\n",
    "    qual_results.append({'model': m['name'], 'BLEU': bleu_score, 'ROUGE-L': rouge_score})\n",
    "df_qual = pd.DataFrame(qual_results)\n",
    "df_qual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19565ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация качества\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.barplot(data=df_qual.melt(id_vars='model', var_name='metric', value_name='score'),\n",
    "            x='model', y='score', hue='metric')\n",
    "plt.title('Quality Metrics')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
